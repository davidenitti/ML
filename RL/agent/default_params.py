def get_default(nameenv):
    if nameenv == 'CartPole-v0':
        params = {
            "memsize": 10000, # number of past experience tuples to store for learning
            "randstart": 50,
            "scaleobs": 1.,
            "policy":False, # policy gradient
            "discounted_policy_grad": False, # use discounted policy gradient formula
            'doubleQ':False, # double Q learning (used when policy=False)
            "clip": -100.0, # gradient clip, negative values mean no gradient clip
            "cliptype": "deltaclip",
            "testeps": 0.,
            "actevery": 1,
            "copyQ": -1, # >=1 when policy=False
            "scalereward": 1.,
            "limitreward": None,
            "probupdate": 1.0,
            "lambda": 0., # 0.0 when policy=False
            "past": 0,
            "entropy":0.0,
            "eps": 0.6,  # Epsilon in epsilon greedy policies
            "mineps": 0.01,
            "decay": 0.995,  # Epsilon decay in epsilon greedy policies
            "initial_learnrate": 0.0005,
            "decay_learnrate": 0.999,
            "decayoptimizer": 0.98,
            "epsoptimizer": 0.00001,
            "discount": 0.98,
            "batch_size": 64,
            "episodic": True,
            "hiddenlayers": [100],
            "regularization": [0.000000, 0.000000],
            "momentum": 0.,
            "file": "",
            "activation": 'elu', # tanh, sigmoid, relu, elu
            "conv":False,
            "fullgrad":False,
            "diffstate": False,
            "convbias":0.1,
            "scaling": 'none',
            "seed": -1,
            "threads": 0,
            "delay_threads": 0.0001,
            "initializer":'variance_scaling', # fixed, variance_scaling
            "shareallnet":True,
            "gpu_fraction": 0.45
            }
    elif nameenv == 'Acrobot-v1':
        env.reward_range = (-1., 0.)
        params = {
            "memsize": 20000,
            "randstart": 10,
            "scaleobs": 1.,
            "limitreward": None,
            "policy":False,
            "discounted_policy_grad": False,
            'doubleQ':False,
            "clip": -1.0,
            "cliptype": "deltaclip",
            "actevery": 1,
            "entropy": 0.01,
            "copyQ": -1,
            "scalereward": 1.0,
            "probupdate": .5,
            "lambda": 0.,
            "testeps": 0.,
            "past": 0,
            "eps": 0.4,  # Epsilon in epsilon greedy policies
            "decay": 0.996,  # Epsilon decay in epsilon greedy policies
            "initial_learnrate": 0.0005,
            "ratio_policy_learnrate": 1,
            "decay_learnrate": 0.9995,
            "decayoptimizer": 0.99,
            "epsoptimizer": 0.000001,
            "mineps": 0.,
            "discount": 0.99,
            "batch_size": 64,
            "episodic": True,
            "iterations": 1,
            "hiddenlayers": [300],
            "regularization": [0.000000, 0.000000],
            "momentum": 0.,
            "file": "",
            "activation": 'elu',
            "conv":False,
            "fullgrad":False,
            "diffstate": False,
            "scaling": 'none',
            "seed": 1,
            "threads": 0,
            "delay_threads":0.0001,
            "initializer": 'variance_scaling',
            "shareallnet": True}
    elif nameenv == 'LunarLander-v2':
        params = {
            "exploss":0.0,
            "initial_learnrate": 0.0008,
            "decay_learnrate": 1,
            "lambda": 0.1,
            "eps": 0.5,  # Epsilon in epsilon greedy policies
            "mineps": 0.02,
            "testeps": 0.,
            "decay": 0.995,  # Epsilon decay in epsilon greedy policies
            "batch_norm" : False,
            "memsize": 150000,
            "randstart": 100,
            "replace": [],#"momentum", "scalereward", "actevery", "probupdate", "lambda", "initial_learnrate", "eps","actevery", "batch_size", "iterations", "memsize"],
            "policy":False,
            "discounted_policy_grad": False,
            'doubleQ':False,
            "clip": -1.0,
            "cliptype": "deltaclip",
            "actevery": 1,
            "copyQ": -1,
            "scalereward": 1.,
            "scaleobs": 1.,
            "limitreward": None,
            "probupdate": 1,

            "entropy":0.01,
            "past": 0,
            "decayoptimizer":0.95,
            "epsoptimizer": 1e-5,
            "discount": 0.99,
            "batch_size": 75,
            "episodic": True,
            "hiddenlayers": [300],
            "regularization": [0.0000, 0.000],
            "momentum": 0.,
            "file": None,
            "activation": 'tanh',
            "conv":False,
            "fullgrad":False,
            "diffstate": False,
            "scaling": 'none',
            "seed": 4,
            "threads": 0,
            "delay_threads": 0.0001,
            "initializer": 'variance_scaling',
            "shareallnet": True
            }
    elif nameenv == 'Breakout-v0':
        params = {
            "memsize": 1000000,
            "randstart": 5000,
            "policy":False,
            "discounted_policy_grad":False,
            "scalereward": 1.,
            "scaleobs": 1./255.,
            "limitreward": [-1., 1.],
            'doubleQ':False,
            "clip": -1.,
            "cliptype": "deltaclip",
            "actevery": 1,
            "copyQ":-1,
            "probupdate": 1.,
            "lambda": 0.,
            "entropy": 0.01,
            "episodic":False,
            "past": 3,
            "decayoptimizer":0.99,
            "epsoptimizer": 0.00001,
            "eps":  .99,  # Epsilon in epsilon greedy policies
            "mineps": 0.1,
            "testeps": 0.05,
            "decay": 0.9996,  # Epsilon decay in epsilon greedy policies
            "initial_learnrate": 0.0002,
            "ratio_policy_learnrate": 1,
            "final_learnrate": 0.0001,
            "decay_learnrate": 1,
            "discount": 0.95,
            "batch_size": 32,
            "hiddenlayers": [256],
            "regularization": [0.0000000,0.0000000,0.0000000],
            "activation":'elu',
            "convbias":0.1,
            "convscale": 0.01,
            "convlayers": [[8, 8, 4, 16], [4, 4, 2, 32]],
            "momentum": 0.,
            "scaling":'crop',
            "dimdobs":(84,84,1),
            "file": "",
            "conv":True,
            "diffstate":False,
            "seed": -1,
            "gpu_fraction":0.24,
            "threads": 1,
            "delay_threads": 0.00001,
            "initializer": 'fixed',
            "shareallnet": True}
    elif nameenv == 'Pong-v0':
        params = {
            "memsize": 2000,
            "storememory": False,
            "memoryfile": "/tmp/Pong2",
            "memorydir": "/tmp/memory",
            "replace": [],#"copyQ","memsize","initial_learnrate","decay_learnrate"],#"discount","episodic","activation","file","storememory","momentum","scalereward","probupdate","lambda","initial_learnrate","eps","batch_size","iterations","memsize"],
            "scalereward": 1.,
            "actevery": 1,
            "copyQ": 10,
            "probupdate": .02,
            "lambda": 0.1,
            "episodic":False,
            "past": 1,
            "eps": 1,  # Epsilon in epsilon greedy policies
            "decay": 0.999,  # Epsilon decay in epsilon greedy policies
            "initial_learnrate": 0.0001,
            "decay_learnrate": 0.9999,
            "discount": 0.99,
            "batch_size": 32,
            "iterations": 1,
            "hiddenlayers": [9,9,450],
            "regularization": [0.000000001],
            "activation":'tanh',
            "momentum": 0.8,
            "file": "pong-model_new15",
            "conv":True,
            "diffstate":False,
            "seed": 1}
    else:
        raise NotImplemented
    return params